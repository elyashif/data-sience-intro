{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b613a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from PIL import Image\n",
    "import io\n",
    "import requests\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.common.exceptions import ElementClickInterceptedException\n",
    "import time     \n",
    "import os        \n",
    "import bs4\n",
    "from bs4 import BeautifulSoup \n",
    "import scipy as sc\n",
    "import numpy as np\n",
    "import requests\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f31c11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_url = 'https://www.tripadvisor.com'\n",
    "url ='https://www.tripadvisor.com/Attractions-g187768-Activities-oa0-Italy.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46c74b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver = webdriver.Chrome('chromedriver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0df865dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome('chromedriver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d02143f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recives the url(string)\n",
    "# returns the html code of the url in Beutifulsoup obj(html.parser)\n",
    "\n",
    "def Load_the_code(url):\n",
    "    driver.get(url)\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source,'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6400c83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recives soup object of trip advisor(first) page \n",
    "# returns the num(int) of pages we need to scrap\n",
    "def Find_how_much_results(soup1):\n",
    "    s = soup1.find('div',class_=\"Ci\")\n",
    "    test_string = s.text\n",
    "    test_string = test_string.replace(',','')\n",
    "    arr = re.findall(r'\\d+', test_string)\n",
    "    arr = [int(i) for i in arr]\n",
    "    num = max(arr)\n",
    "    if num <=30:\n",
    "        return 1;\n",
    "    else:\n",
    "        return (num//30) +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff070156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recives objective of soup and extracts the \"next page\" link\n",
    "# returns the link(string)\n",
    "def Find_the_next_link(soup2):\n",
    "    obj = soup2.find_all(\"a\",class_=\"BrOJk u j z _F wSSLS tIqAi unMkR\")\n",
    "    if obj[0]['aria-label'] == 'Previous page':\n",
    "        return obj[1]['href']\n",
    "    else:\n",
    "        return obj[0]['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fcd1cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of all attractions from the url:\n",
    "\n",
    "def get_list_of_attrs_links_from_page(soup_obj):\n",
    "    list_of_atters_links=[]\n",
    "    eror_count = 0\n",
    "    all_attrractions_raw_code_list = soup_obj.find_all(\"div\",class_=\"alPVI eNNhq PgLKC tnGGX\")\n",
    "    for attr in all_attrractions_raw_code_list:\n",
    "        try:\n",
    "            l=attr.find(\"a\")\n",
    "            list_of_atters_links.append(\"https://www.tripadvisor.com/\"+l['href'])\n",
    "        except:\n",
    "            eror_count+=1\n",
    "            continue\n",
    "        \n",
    "    return list_of_atters_links,eror_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5a3c4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracts the name of the attraction\n",
    "\n",
    "def Extract_name_of_attraction(soup_obj):\n",
    "    try:\n",
    "        res=soup_obj.find(\"h1\",class_=\"biGQs _P fiohW eIegw\")\n",
    "        res = res.string\n",
    "        return res\n",
    "    except:\n",
    "        return np.nan\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ac9a953",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Extract_tag_1_and_tag_2(soup_obj):\n",
    "    try:\n",
    "        name = soup_obj.find(\"div\",class_=\"fIrGe _T bgMZj\")\n",
    "        st=name.string\n",
    "        return st\n",
    "        try:\n",
    "            x =re.findall(r\"\\w+[\\s\\w&]+\",st)\n",
    "            if len(x) >1:\n",
    "                return x[0],x[1]\n",
    "            else:\n",
    "                return x[0],np.nan\n",
    "                \n",
    "        except:\n",
    "            return np.nan,np.nan\n",
    "    except:\n",
    "            return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc641ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Extract_num_of_reviewers(soup_obj):\n",
    "    try:\n",
    "        name = soup_obj.find(\"span\",class_=\"yyzcQ\")\n",
    "        temp_string=name.string\n",
    "        temp_string =temp_string.replace(\",\",\"\")\n",
    "        temp_int = int(temp_string)\n",
    "        return temp_int\n",
    "    except:\n",
    "        return np.nan\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb3e2c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Extract_Rank_score(soup_obj):\n",
    "    try:\n",
    "        rank = soup_obj.find(\"div\",class_='biGQs _P fiohW hzzSG uuBRH')\n",
    "        rank = rank.string\n",
    "        rank = float(rank)\n",
    "        return rank\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e7106e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Extract_num_of_resturants_nearby(soup_obj):\n",
    "    try:\n",
    "        num_of_rests = soup_obj.find(\"div\",class_='biGQs _P pZUbB mowmC KxBGd')\n",
    "        num_of_rests = num_of_rests.string\n",
    "        num_of_rests = re.search(r\"\\d[\\W\\d]+\",num_of_rests)\n",
    "        num_of_rests = num_of_rests.group(0)\n",
    "        num_of_rests = num_of_rests.replace(r\",\",\"\")\n",
    "        return num_of_rests\n",
    "    except:\n",
    "        return np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3949c7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Extract_num_of_attractions_nearby(soup_obj):\n",
    "     try:\n",
    "        num_of_att = soup_obj.find_all(\"div\",class_='biGQs _P pZUbB mowmC KxBGd')\n",
    "        num_of_att = num_of_att[1].string\n",
    "        num_of_att = re.search(r\"\\d[\\W\\d]+\",num_of_att)\n",
    "        num_of_att = num_of_att.group(0)\n",
    "        num_of_att = num_of_att.replace(\",\",\"\")\n",
    "        num_of_att = num_of_att.replace(\" \",\"\")\n",
    "        return num_of_att\n",
    "     except:\n",
    "        return np.nan\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d8e80e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Extract_destric(soup_obj):\n",
    "    try:\n",
    "        f = soup_obj.find_all(\"span\",class_='biGQs _P XWJSj Wb')\n",
    "        f = [i.string for i in f]\n",
    "        for i in f:\n",
    "            j = re.search(\"Italy\",i)\n",
    "            if j != None:\n",
    "                j=i\n",
    "                break\n",
    "        return j\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e73f7e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Extract_visit_time(soup_obj):\n",
    "     try:\n",
    "        time = soup_obj.find(\"div\",class_='_c')\n",
    "        time = time.string\n",
    "        return time\n",
    "     except:\n",
    "        return np.nan\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42cbb756",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Extract_opening_houres(soup_obj):\n",
    "    try:\n",
    "        opening_houres = soup_obj.find(\"span\",class_='EFKKt')\n",
    "        opening_houres = opening_houres.string\n",
    "        return opening_houres\n",
    "    except:\n",
    "        return np.nan\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1970ed49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_of_primary_urls = pd.read_csv('new_1500')\n",
    "df_of_primary_urls = df_of_primary_urls['page_url']\n",
    "df_of_primary_urls = df_of_primary_urls.iloc[50:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cf32ef12",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_name=[]\n",
    "tag_1=[]\n",
    "# tag_2=[]\n",
    "total_reviewers=[]\n",
    "rank_list = []\n",
    "num_of_resturants_nearby=[]\n",
    "num_of_attractions_nearby=[]\n",
    "opening_houres =[]\n",
    "recomended_amount_time_to_visit=[]\n",
    "distric_city=[]\n",
    "\n",
    "\n",
    "\n",
    "df_of_primary_urls = pd.read_csv('new_1500')\n",
    "df_of_primary_urls = df_of_primary_urls['page_url']\n",
    "df_of_primary_urls = df_of_primary_urls.iloc[900:900]\n",
    "\n",
    "\n",
    "for ur in df_of_primary_urls:\n",
    "    try:\n",
    "        driver.get(ur)\n",
    "        code = driver.page_source\n",
    "        soup = BeautifulSoup(code,'html.parser')\n",
    "        time.sleep(5)\n",
    "        list_of_att_url,eror_count = get_list_of_attrs_links_from_page(soup)\n",
    "    except:\n",
    "        driver = webdriver.Chrome('chromedriver')\n",
    "        driver.get(ur)\n",
    "        code = driver.page_source\n",
    "        time.sleep(5)\n",
    "        soup = BeautifulSoup(code,'html.parser')\n",
    "        list_of_att_url,eror_count = get_list_of_attrs_links_from_page(soup)\n",
    "        \n",
    "        \n",
    "\n",
    "# ### ------ this part is for extracting only one page of results---------------\n",
    "\n",
    "\n",
    "    for url in list_of_att_url:\n",
    "        try:\n",
    "\n",
    "            try:\n",
    "                driver.get(url)\n",
    "                time.sleep(5)\n",
    "            except:\n",
    "                driver = webdriver.Chrome('chromedriver')\n",
    "                time.sleep(5)\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                soup = driver.page_source\n",
    "                obj = BeautifulSoup(soup,'html.parser')\n",
    "            except:\n",
    "                print(f\" stoped at {url}\")\n",
    "                break\n",
    "            name = Extract_name_of_attraction(obj) \n",
    "            tag1= Extract_tag_1_and_tag_2(obj)\n",
    "            num_of_rev=Extract_num_of_reviewers(obj)\n",
    "            rank=Extract_Rank_score(obj)\n",
    "            rest=Extract_num_of_resturants_nearby(obj)\n",
    "            att = Extract_num_of_attractions_nearby(obj)\n",
    "            dest = Extract_destric(obj)\n",
    "            opening = Extract_opening_houres(obj)\n",
    "            visit_time = Extract_visit_time(obj)\n",
    "\n",
    "            tag_1.append(tag1)    \n",
    "            site_name.append(name)\n",
    "            total_reviewers.append(num_of_rev)\n",
    "            rank_list.append(rank)\n",
    "            num_of_resturants_nearby.append(rest)\n",
    "            num_of_attractions_nearby.append(att)\n",
    "            recomended_amount_time_to_visit.append(visit_time)\n",
    "            distric_city.append(dest)\n",
    "            opening_houres.append(opening)\n",
    "\n",
    "        except:\n",
    "            print(f\" stooped at {url}\")\n",
    "    ### --------------------------------------------------------------------------  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c83a0c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\"site_name\":site_name,\"tag_1\":tag_1,\"distric_city\":distric_city,\"total_reviewers\":total_reviewers,\n",
    "     \"rank\":rank_list,\"num_of_resturants_nearby\":num_of_resturants_nearby,\n",
    "     \"num_of_attractions_nearby\":num_of_attractions_nearby\n",
    "     ,\"recomended_amount_time_to_visit\":recomended_amount_time_to_visit\n",
    "     ,\"opening_houres\":opening_houres}\n",
    "\n",
    "curr_iteration_30_res = pd.DataFrame(d)\n",
    "curr_iteration_30_res.to_csv('first_900_950')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
